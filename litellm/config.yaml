# LiteLLM Proxy: all LLM/embedding requests from the app go through this proxy.
# Only Minimax and Featherless are configured here; the app uses LITELLM_* in env.
#
# Install: pip install 'litellm[proxy]'
# Run: bun run proxy (in one terminal); bun dev (in another).
# Default port: 4000. Set LITELLM_EMBEDDING_API_BASE=http://localhost:4000 in env.
#
# Proxy env (no OpenAI connection - keys are loaded when you run "bun run proxy"):
#   For minimax-embed: MINIMAX_API_KEY, MINIMAX_GROUP_ID, OPENAI_API_KEY=$MINIMAX_API_KEY
#   For featherless-embed: FEATHERLESS_API_KEY, OPENAI_API_KEY=$FEATHERLESS_API_KEY

model_list:
  # Minimax embedding (embo-01)
  - model_name: minimax-embed
    litellm_params:
      model: openai/embo-01
      api_base: https://api.minimax.chat/v1
      api_key: os.environ/MINIMAX_API_KEY
    model_info:
      mode: embedding

  # Featherless embedding (OpenAI-compatible; api_base points to Featherless)
  - model_name: featherless-embed
    litellm_params:
      model: openai/text-embedding-3-small
      api_base: https://api.featherless.ai/v1
      api_key: os.environ/FEATHERLESS_API_KEY
    model_info:
      mode: embedding

general_settings:
  master_key: sk-1234
