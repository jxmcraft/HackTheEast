# LiteLLM Proxy: all LLM/embedding requests from the app go through this proxy.
# Only Minimax and Featherless are configured here; the app uses LITELLM_* in env.
#
# Install: pip install 'litellm[proxy]'
# Run: bun run proxy (in one terminal); bun dev (in another).
# Default port: 4000. Set LITELLM_EMBEDDING_API_BASE=http://localhost:4000 in env.
#
# Proxy env (no OpenAI connection – keys are used only for Minimax/Featherless):
#   When you run "bun run proxy", keys are loaded from .env and .env.local in the project root.
#   For minimax-embed set: MINIMAX_API_KEY, MINIMAX_GROUP_ID, and OPENAI_API_KEY=$MINIMAX_API_KEY
#   For featherless-embed set: FEATHERLESS_API_KEY and OPENAI_API_KEY=$FEATHERLESS_API_KEY
#   (OPENAI_API_KEY is required by LiteLLM's internal client; requests still go to api_base above.)

model_list:
  # Minimax embedding (embo-01) – requests go to Minimax only
  - model_name: minimax-embed
    litellm_params:
      model: openai/embo-01
      api_base: https://api.minimax.chat/v1
      api_key: os.environ/MINIMAX_API_KEY
    model_info:
      mode: embedding

  # Featherless embedding – requests go to Featherless only
  - model_name: featherless-embed
    litellm_params:
      model: openai/text-embedding-3-small
      api_base: https://api.featherless.ai/v1
      api_key: os.environ/FEATHERLESS_API_KEY
    model_info:
      mode: embedding

general_settings:
  master_key: sk-1234
